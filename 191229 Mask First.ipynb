{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 191229 Mask First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## ideas\n",
    "- [ ] 意识世界的维度可能是无穷多的，我们一般使用有限数量的、预定义的维度来加以模拟. 一个关键的点是，我们应该只使用最多固定数量8个维度来表示真实世界维度的一个视图view. 最核心的问题在于:\n",
    "    - 新维度的发现机制\n",
    "    - 不同agent的不同维度的统一，合并机制，好互相交流，模拟人类语言、手势协议的过程\n",
    "    - 最终有个library（知识图谱）来统一合并这些视图，从而尽可能地模拟真实世界，\n",
    "    - 西部世界的蜂巢互叠有点像卷积中的互相重合，把pixel和周围的8个pixel对比，掌握差模式之后，就统一为一个整体（pooling降维）\n",
    "\n",
    "- [ ] 对target或label做softmax，或者让1为0.8，0积分起来等于0.2，类似于一个$\\frac{0.2}{|x-x_0|}$\n",
    "- [x] batch norm是对数据进行归一化。从余弦相似性的角度来说，也可以只对参数进行归一化。在每次optimizer step完后给每个filter除以它的模长。一般normalization是说减mean除variance.余弦相似度是从向量的角度搞得，减mean除variance会改变原始数据的向量方向，变成从聚类中心指向数据， 余弦相似度不会改变原始方向，但会缺少聚类的信息\n",
    "    - ![余弦相似度](../others/imgs/日记/cossimilarity.jpg)\n",
    "- [ ] plan 是规划， 而不是我想要，是我觉得我大概可以搞那个事情，然后对行动和规划都打分。规划有可行性和明智性。可行性不一定是规划的问题，还可能是行动者能力问题；明智性决定最终逼近最优解。**规划要有封装，向上提供接口，服务高层规划的能力**。\n",
    "- [ ] \n",
    "      1. 根据超模糊色块(可以说saliency map)确定初始观察点(们), 然后扩散着去找线条(或梯度sobel)高于阈值的像素位置, 去给像素两侧打label;\n",
    "      2. 现在是给点(pixel距离为1), 后面可以拓展到pixel距离为10, 100的情况, 来避免渐变的问题;\n",
    "      3. 其实阈值应当由高0.95倍而低0.05倍, 这样符合由粗而细的直觉;\n",
    "- [ ] 一种图卷积, 第一层以3\\*3的kernel为基础(其实3\\*3好的原因其实是因为它有明确的方向侧重), 通过一部分10几个手工设计的filter(包括坐标, canny, sobel, laplace, fft, gradient等)+ 自优化的filters来提取3\\*3小范围特征, relu激活, 然后重复两三层, 后面就开始更大范围的扫街, 比如11\\*11的核内分8个方向, 每个方向relu_pool一个最大的响应, 得到其位置, 计算距离中心的位置, 8个位置的比例可以用来当做特征, 特定模型的参数会跟与之对应的特征产生最大共鸣响应\n",
    "- [ ] 类内与类外的较量, 类的凝聚力, 类内元素的不合群力(觉得自己与其他成员不同), 与法律的约束力(离群会带来强烈惩罚)之间的较量, 对于每个个体来说, 它同时评估各个声索方及转换成本(计算力充足的情况下应为0, 但为了节省算力而设置一个门槛)的score, 决定归属谁\n",
    "- [ ] 特征数量不匹配是造成识别困难的巨大阻碍，从这些特征中筛选关键特征很重要，不如让这些特征竞争，只给5到10个特征槽位，让这些特征竞争它们的特征重要度，从而方便比较与识别，具体环境再加载具体的特征槽位，就像RPN一样，搞一个网络来确定槽位场景\n",
    "- [ ] 用数据结构模拟CNN卷积的过程,先试着重现, 核心的问题是针对pooling的格子必选问题, 改为区域最大, 从而不漏掉局部最优, 也不多加局部最优进来, 但是卷积核的参数BP要好好想想\n",
    "- [ ] 对于一张图(可以是灰度图), 找出各个通道(rgblab)在局部的变化最强的点(一阶导数为0)、当前色彩变化的极值点(最亮或最暗,二阶导数为0)\n",
    "    - 可能不是方差，而是3*3 5*5的一阶二阶梯度来表达光滑与粗糙的概念，梯度方向的局部一致度的方差倒是一个利器来提取边界和曲线，至少是判断是否值得搞提取线条的依据\n",
    "    - 现有的方法把梯度正交方向的值加起来了，不合理，应当分通道处理，一个有意思的处理应当是找到一对正交的梯度对，它们所表达的梯度相对x方向的角度是特征，讲道理一个强响应，一个弱为0的话，只保留那个强响应的也就够了，回头把目前weight所表达的梯度方向打印一下看一看\n",
    "- [ ] 使用RL(DDPG)来选择channel, 选出来的顺序按照1/2,1/4,1/8,1/8的类似指数衰减的加权来组合, 选的时候用编码器,但应该是循环编码之类的编码\n",
    "- [ ] 针对SLIC的情况,模拟CNN的滑窗, 从细到粗给出聚类, top一层使用旗下的众数的top3作为自己的特征, 进一步聚类, 一个点对多个周边的簇打分, 取分最高的\n",
    "- [ ] 有编号的空间位置(0～1或给定了最大wh)和有序的通道，有了远近概念后，适当的平方层也要有。考虑单层fine tune block\n",
    "- [ ] 学会使用图像语言来表达异同，对于同类的，就应该把中心的像素的值拿来当做自己的特征之一，觉得相近的，在一定阈值或者或者几个限定之内的前几相似的像素都应该拿来当做自己的特征，以有意义地表达近的概念。这样做主要是因为图像当中似乎没有明确表达坐标的概念，当然也可以反过来搞出一个明确表达坐标的表示法\n",
    "- [ ] channel之间,或者卷积核之间的施密特正交化\n",
    "- [ ] 这个xception有点EM的意思, 可以考虑使用EM给他表示, 打印展示, 甚至改进[9.30]\n",
    "- [ ] 关于xception的神奇疗效, 关于如何在更好处理pooling和stride各自带来的问题,考虑一种在最大,最小,第二大,第二小池上使用4入1出1\\*1卷积加权的做法, 都有评价参数,但只有一个参与运算\n",
    "- [ ] 卷积是怎么评价\"这个特征图的这个地方跟那个啥有多像\"这个问题的, 猜是卷积核的样子,比如说连续几层的卷积层的filter的组合\n",
    "- [ ] 使用色彩, erosion dilation, 简化图像信息, 优化颜色位置编码\n",
    "- [ ] 使用cycle gan生成mask的auto code, 使用drl解释code\n",
    "- [ ] mask rcnn的结果进行ppo, roialign或rpn之后的特征处开始训练actor critic, 看看最终能不能帮助改善rcnn, 主要是为了提高iou\n",
    "- [ ] 球坐标经纬度范围加zoom信息及其归一化与置信度，经纬度上的高斯，随时间方差增大\n",
    "- [ ] region proposal with hsv 记录邻域, 宽阈值筛选， 然后再精细化阈值， 辨认脸型\n",
    "- [ ] 图像的小片方差也有它的边缘信息, 即, 两个预处理, LAB转换和方差map\n",
    "\n",
    "## 今日任务\n",
    "- [ ] 测试lookaround\n",
    "- [ ] 整合进super pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2, time, torch\n",
    "os.chdir('/home/xianr/TurboRuns/CenterMask')\n",
    "from glob import glob\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "\n",
    "config_file = 'run/try_2/new_config.yml'\n",
    "cfg.merge_from_file(config_file)\n",
    "cfg.merge_from_list(['MODEL.WEIGHT', 'run/try_2/model_0010000.pth'])\n",
    "# cfg.MODEL.WEIGHT = args.weights\n",
    "cfg.freeze()\n",
    "\n",
    "from maskfirst.do_train_net import MaskFirst, InstancePyramid\n",
    "from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\n",
    "from maskrcnn_benchmark.modeling.roi_heads.mask_head.inference import Masker\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as F_transform\n",
    "from maskrcnn_benchmark import layers as L\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.colormap import colormap, COLORS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from maskrcnn_benchmark.structures.keypoint import PersonKeypoints\n",
    "from maskrcnn_benchmark.utils import cv2_util\n",
    "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
    "import torch.nn.functional as F\n",
    "from maskfirst.predictor import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_val2014_000000000139 processing...\n"
     ]
    }
   ],
   "source": [
    "img_path = 'datasets/coco/val2014/COCO_val2014_000000000139.jpg'\n",
    "output_dir = 'run/jupyter/'\n",
    "\n",
    "if os.path.isfile(img_path):\n",
    "    imglist = [img_path]\n",
    "else:\n",
    "    imglist = glob(os.path.join(img_path, '*'))\n",
    "\n",
    "original_image = cv2.imread(imglist[0])\n",
    "assert original_image is not None\n",
    "im_name, _ = os.path.splitext(os.path.basename(imglist[0]))\n",
    "print(f\"{im_name} processing...\")\n",
    "start_time = time.time()\n",
    "# composite = one_shot_plot(img)\n",
    "# composite = coco_demo.run_on_opencv_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply pre-processing to image\n",
    "model = MaskFirst(cfg)\n",
    "model.eval()\n",
    "device = torch.device(cfg.MODEL.DEVICE)\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "masker = Masker(threshold=0.2, padding=1)\n",
    "\n",
    "# image = self.transforms(original_image)\n",
    "if cfg.INPUT.TO_BGR255:\n",
    "    to_bgr_transform = T.Lambda(lambda x: x * 255)\n",
    "else:\n",
    "    to_bgr_transform = T.Lambda(lambda x: x[[2, 1, 0]])\n",
    "\n",
    "normalize_transform = T.Normalize(\n",
    "    mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD\n",
    ")\n",
    "min_size = cfg.INPUT.MIN_SIZE_TEST\n",
    "max_size = cfg.INPUT.MAX_SIZE_TEST\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.ToPILImage(),\n",
    "        Resize(min_size, max_size),\n",
    "        T.ToTensor(),\n",
    "        to_bgr_transform,\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")\n",
    "image = transform(original_image)\n",
    "\n",
    "# convert to an ImageList, padded so that it is divisible by\n",
    "# cfg.DATALOADER.SIZE_DIVISIBILITY\n",
    "image_list = to_image_list(image, cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "image_list = image_list.to(device)\n",
    "# compute predictions\n",
    "with torch.no_grad():\n",
    "    pyramids_list = model(image_list)\n",
    "\n",
    "valid_pyramids = [p for p in pyramids_list[0] if p.get_root_response(2) > 0.5]\n",
    "masks = torch.cat([i_p.get_mask(2) for i_p in valid_pyramids], dim=0).to(cpu_device)\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "height, width = original_image.shape[:-1]\n",
    "masks_scale = F.interpolate(masks, (height, width), mode='bilinear', align_corners=False)\n",
    "masks_bool = masks_scale > 0.2\n",
    "\n",
    "fake_box = torch.tensor([[100, 50, 200, 150]]*len(masks_bool), device=masks_bool.device)\n",
    "box_fake = BoxList(fake_box, tuple(masks_bool.shape[-2:]), mode=\"xyxy\")\n",
    "box_fake.add_field('mask', masks_bool)\n",
    "keep = masks_scale.view(len(masks_scale), -1).max(dim=1)\n",
    "\n",
    "# top_predictions = self.select_top_predictions(predictions)\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "result = original_image.copy()\n",
    "# result = self.overlay_mask(result, box_fake)\n",
    "masks_np = box_fake.get_field(\"mask\").numpy()\n",
    "\n",
    "colors = colormap(rgb=True).tolist()\n",
    "\n",
    "mask_img = np.copy(original_image)\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# for mask, color in zip(masks_np, colors):\n",
    "for mask, color in zip(masks_np[:1], colors):\n",
    "    # color_mask = color_list[color_id % len(color_list)]\n",
    "    # color_id += 1\n",
    "\n",
    "    # thresh = mask[0, :, :, None]\n",
    "    thresh = mask[0, :, :, None].astype(np.uint8)\n",
    "    contours, hierarchy = cv2_util.findContours(\n",
    "        thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    mask_img = cv2.drawContours(mask_img, contours, -1, color, -1)\n",
    "\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "# composite = image\n",
    "alpha = 0.45\n",
    "composite = cv2.addWeighted(original_image, 1.0 - alpha, mask_img, alpha, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO_val2014_000000000139\tinference time: 424.49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"{}\\tinference time: {:.2f}s\".format(im_name, time.time() - start_time))\n",
    "save_path = os.path.join(output_dir, f'{im_name}_result.jpg')\n",
    "cv2.imwrite(save_path, composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=6.75s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=2.76s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from maskrcnn_benchmark.data import make_data_loader\n",
    "\n",
    "data_loader = make_data_loader(\n",
    "    cfg,\n",
    "    is_train=True,\n",
    "    is_distributed=False,\n",
    "    start_iter=0,\n",
    ")\n",
    "images, targets, _ = next(iter(data_loader))\n",
    "images = images.to(device)\n",
    "targets = [target.to(device) for target in targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model\n",
    "self.train()\n",
    "x_img = images.tensors\n",
    "xs_r50 = self.r50(x_img)\n",
    "fs_fpn = self.fpn(xs_r50)\n",
    "N, _, img_size_h, img_size_w = x_img.shape\n",
    "device = x_img.device\n",
    "level_sizes = [tuple(f.shape[-2:]) for f in fs_fpn[::-1]]\n",
    "\n",
    "losses = {}\n",
    "losses_0 = []\n",
    "losses_1 = []\n",
    "losses_2 = []\n",
    "losses_3 = []\n",
    "losses_4 = []\n",
    "test_masks = []\n",
    "# for i in range(N):\n",
    "i = 0\n",
    "\n",
    "if self.training:\n",
    "    target_levels = self._init_target((img_size_h, img_size_w ), device, targets[i])\n",
    "\n",
    "curr_level = 0\n",
    "x_curr = fs_fpn[::-1][curr_level]\n",
    "init_pos = torch.nonzero(torch.ones_like(x_curr[0][0]))\n",
    "inst_pyramids = [InstancePyramid(pos, curr_level, level_sizes) for pos in init_pos]\n",
    "self.compute_mask(curr_level, x_curr[[i]], inst_pyramids, True)\n",
    "\n",
    "curr_level = 1\n",
    "x_curr = fs_fpn[::-1][curr_level]\n",
    "# import pdb; pdb.set_trace()\n",
    "self.compute_mask(curr_level, x_curr[[i]], inst_pyramids)\n",
    "new_masks = torch.cat([i_p.get_mask(curr_level) for i_p in inst_pyramids], dim=1)\n",
    "new_pos = torch.nonzero(new_masks[0].max(dim=0)[0] < self.low_thresh)\n",
    "new_pyramids = [InstancePyramid(pos, curr_level, level_sizes) for pos in new_pos]\n",
    "self.compute_mask(curr_level, x_curr[[i]], new_pyramids, True)\n",
    "inst_pyramids += new_pyramids\n",
    "\n",
    "curr_level = 2\n",
    "x_curr = fs_fpn[::-1][curr_level]\n",
    "# import pdb; pdb.set_trace()\n",
    "self.compute_mask(curr_level, x_curr[[i]], inst_pyramids)\n",
    "new_masks = torch.cat([i_p.get_mask(curr_level) for i_p in inst_pyramids], dim=1)\n",
    "new_pos = torch.nonzero(new_masks[0].max(dim=0)[0] < self.low_thresh)\n",
    "new_pyramids = [InstancePyramid(pos, curr_level, level_sizes) for pos in new_pos]\n",
    "self.compute_mask(curr_level, x_curr[[i]], new_pyramids, True)\n",
    "inst_pyramids += new_pyramids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if self.training:\n",
    "    losses_0.append(self.compute_loss(curr_level, inst_pyramids, target_levels))\n",
    "    losses_1.append(self.compute_loss(curr_level, inst_pyramids, target_levels))\n",
    "    losses_2.append(self.compute_loss(curr_level, inst_pyramids, target_levels))\n",
    "else:\n",
    "    test_masks.append(inst_pyramids)\n",
    "\n",
    "\n",
    "\n",
    "losses['level_0']= sum(loss for loss in losses_0)\n",
    "losses['level_1']= sum(loss for loss in losses_1)\n",
    "losses['level_2']= sum(loss for loss in losses_2)\n",
    "losses['level_3']= sum(loss for loss in losses_3)\n",
    "losses['level_4']= sum(loss for loss in losses_4)\n",
    "# return losses if self.training else test_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inst_pyramids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid = inst_pyramids[0]\n",
    "mask = pyramid.get_mask(2)\n",
    "target = target_levels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef98d0df98>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(target[0][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5502f75c5ff341a4a2beeba84e7131d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=13, description='c', max=26), Output()), _dom_classes=('widget-interact'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(c=(0, target.shape[1]-1))\n",
    "def interactshow(c):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(target[0][c].detach().cpu().numpy())\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(target[0][:c+1].sum(0).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fef97362400>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(target[0][9].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 19])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0][:3].sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py365]",
   "language": "python",
   "name": "conda-env-py365-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
