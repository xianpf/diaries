{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019/04/19\n",
    "## ideas\n",
    "- [x] 参数归一化，梯度下降也要归一化，normalization的在线版，一次直接分配到位，使得梯度更新之后所有weight的和依然为1, 观察考虑HSV,保存历史数据\n",
    "- [ ] 球坐标经纬度范围加zoom信息及其归一化与置信度，经纬度上的高斯，随时间方差增大\n",
    "\n",
    "## 今日任务\n",
    "- [x] 理解清楚optimizer在BP中的具体做法\n",
    "- [x] 理解清楚module的包含内容\n",
    "- [x] 理解清楚parameters\n",
    "- [ ] 搞出参数归一化的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 历史任务\n",
    "- [ ] 把annotation里面的游程编码一条一条在原图上标出来,搞懂,并且会用coco.py给的接口\n",
    "- [ ] 搞明白评价接口,能随意使用判定预测结果的准确率\n",
    "- [x] 开始找DQN pytorch的代码\n",
    "- [ ] 开始找PPO pytorch的代码\n",
    "- [ ] 找到rcnn训练崩溃的原因\n",
    "- [ ] 训练一个神经网络来指导(为instance打分)选择对象以及调整, 考虑bbox大小, 中心位置占画面长宽的比例, \n",
    "- [ ] 确定一个默认的overview视角作为后备\n",
    "- [ ] 设计一个机制,五级打分:没啥变化, 变好了, 变坏了, 太好了, 太坏了, 人只进行这样的评判, 然后可以在线地传递到instance的打分上\n",
    "- [ ] 结合yolov3与pose body, 实现快速识别\n",
    "- [ ] 自动提取object的skeleton信息，然后dqn之，以完善边缘的准确率\n",
    "- [x] 开始探索zoo\n",
    "- [ ] 赋予AI一些记忆(Memory),哪怕7秒的记忆,让前面一段时间识别的东西有留存,并且指导最新的工作.而不是摄像头的机械记忆."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] xtool, 图片显示工具 eg loss的记录与准实时画图，以及GIF图生成\n",
    "- [x] xtool, json数据的动态存储\n",
    "- [x] xtool, 生成训练report工具, 保存历史数据, weight显示工具\n",
    "- [x] 解决zoom与speed的关系问题\n",
    "- [x] 结合yolov3与face detection, 实现快速识别\n",
    "- [x] 写2000字的EE8001\n",
    "- [x] 小改xtool，实现不显示只保存图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 然后开始搞自动生成report的功能。可以一半了\n",
    "- weight的大小或者权重，与其符号之间怎么把握？看绝对值还是符号值？\n",
    "- 考虑上符号问题， 再演算一下和为1还能不能搞， 初步觉得符号存在还能搞\n",
    "- 可以考虑穿个e指数的衣服来逃避符号问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega import VegaLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var spec = {\"data\": {\"values\": [{\"a\": \"A\", \"b\": 28}, {\"a\": \"B\", \"b\": 55}, {\"a\": \"C\", \"b\": 43}, {\"a\": \"D\", \"b\": 91}, {\"a\": \"E\", \"b\": 81}, {\"a\": \"F\", \"b\": 53}, {\"a\": \"G\", \"b\": 19}, {\"a\": \"H\", \"b\": 87}, {\"a\": \"I\", \"b\": 52}]}, \"mark\": \"bar\", \"encoding\": {\"x\": {\"field\": \"a\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}}};\n",
       "var opt = {};\n",
       "var type = \"vega-lite\";\n",
       "var id = \"4f83b0c8-cc9b-47c6-87be-8ab1dfab859d\";\n",
       "\n",
       "var output_area = this;\n",
       "\n",
       "require([\"nbextensions/jupyter-vega/index\"], function(vega) {\n",
       "  var target = document.createElement(\"div\");\n",
       "  target.id = id;\n",
       "  target.className = \"vega-embed\";\n",
       "\n",
       "  var style = document.createElement(\"style\");\n",
       "  style.textContent = [\n",
       "    \".vega-embed .error p {\",\n",
       "    \"  color: firebrick;\",\n",
       "    \"  font-size: 14px;\",\n",
       "    \"}\",\n",
       "  ].join(\"\\\\n\");\n",
       "\n",
       "  // element is a jQuery wrapped DOM element inside the output area\n",
       "  // see http://ipython.readthedocs.io/en/stable/api/generated/\\\n",
       "  // IPython.display.html#IPython.display.Javascript.__init__\n",
       "  element[0].appendChild(target);\n",
       "  element[0].appendChild(style);\n",
       "\n",
       "  vega.render(\"#\" + id, spec, type, opt, output_area);\n",
       "}, function (err) {\n",
       "  if (err.requireType !== \"scripterror\") {\n",
       "    throw(err);\n",
       "  }\n",
       "});\n"
      ],
      "text/plain": [
       "<vega.vegalite.VegaLite at 0x7fd5aa266400>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "jupyter-vega": "#4f83b0c8-cc9b-47c6-87be-8ab1dfab859d"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VegaLite({\n",
    "  \"data\": {\n",
    "    \"values\": [\n",
    "      {\"a\": \"A\",\"b\": 28}, {\"a\": \"B\",\"b\": 55}, {\"a\": \"C\",\"b\": 43},\n",
    "      {\"a\": \"D\",\"b\": 91}, {\"a\": \"E\",\"b\": 81}, {\"a\": \"F\",\"b\": 53},\n",
    "      {\"a\": \"G\",\"b\": 19}, {\"a\": \"H\",\"b\": 87}, {\"a\": \"I\",\"b\": 52}\n",
    "    ]\n",
    "  },\n",
    "  \"mark\": \"bar\",\n",
    "  \"encoding\": {\n",
    "    \"x\": {\"field\": \"a\", \"type\": \"ordinal\"},\n",
    "    \"y\": {\"field\": \"b\", \"type\": \"quantitative\"}\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索到的pytorch moulder信息\n",
    "-  ```net._modules.items()/ net._modules``` 可以看到所有的子模块， 然后再递归地看进去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# net.__dict__\n",
    "{\n",
    "    'weight_layers': [\n",
    "        WNormConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \n",
    "        Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    ], \n",
    "    '_backend': < torch.nn.backends.thnn.THNNFunctionBackend object at 0x7f9068bb3e10 >, \n",
    "    '_parameters': OrderedDict(), \n",
    "    '_buffers': OrderedDict(), \n",
    "    '_backward_hooks': OrderedDict(), \n",
    "    '_forward_hooks': OrderedDict(), \n",
    "    '_forward_pre_hooks': OrderedDict(), \n",
    "    '_modules': OrderedDict(\n",
    "        [\n",
    "            (\n",
    "                'features', \n",
    "                Sequential(\n",
    "                   2(0): WNormConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                   4(1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                   4(2): ReLU(inplace)\n",
    "                   6(3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                   8(4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                   8(5): ReLU(inplace)\n",
    "                   8(6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "                  10(7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                  12(8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  12(9): ReLU(inplace)\n",
    "                  14(10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                  16(11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  16(12): ReLU(inplace)\n",
    "                  16(13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "                  18(14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                  20(15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  20(16): ReLU(inplace)\n",
    "                  22(17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                  24(18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  24(19): ReLU(inplace)\n",
    "                  26(20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                  28(21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  28(22): ReLU(inplace)\n",
    "                    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  32(25): ReLU(inplace)\n",
    "                    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "                    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  36(29): ReLU(inplace)\n",
    "                    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  40(32): ReLU(inplace)\n",
    "                    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  44(35): ReLU(inplace)\n",
    "                    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  48(38): ReLU(inplace)\n",
    "                    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "                    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  52(42): ReLU(inplace)\n",
    "                    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  56(45): ReLU(inplace)\n",
    "                    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  60(48): ReLU(inplace)\n",
    "                    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "                  64(51): ReLU(inplace)\n",
    "                    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "                    (53): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
    "                )\n",
    "            ), \n",
    "            (\n",
    "                'classifier', \n",
    "                Linear(in_features=512, out_features=10, bias=True)\n",
    "            )\n",
    "        ]\n",
    "    ), \n",
    "    'training': True\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 这里主要关注*'_parameters'*这个Odict，可以看到，net层（也就是VGG）中*'_parameters'*是空的，因为在顶层的VGG中没有定义任何参数\n",
    "- ### 同时第二要关注*'_modules'*这个Odict，它里面存放串化的子模块，而这些子模块就和一个个小VGG一样， 他们又有自己的*'_parameters'*和*'_modules'*这些Odict，所以要找全部的*'_parameters'*，也要递归进入子模块里面去看，这样才能找全。\n",
    "```python\n",
    "net._modules['features'][0] # 即为 WNormConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "```\n",
    "- ### 这样手动找还是太慢了， 来看看*Optimizer.py*是怎么找的\n",
    "#### net.parameters()\n",
    "```python\n",
    "list(net.parameters())\n",
    "```\n",
    "没错，就是这么简单！看看输出的结果是啥\n",
    "```python\n",
    "p list(net.parameters())\n",
    "[Parameter containing:\n",
    "tensor([[[[-0.1077,  0.0878, -0.0147],\n",
    "          [ 0.0702,  0.0186,  0.0432],\n",
    "          [-0.0008,  0.1221,  0.0047]],\n",
    "        ...,\n",
    "         [[ 0.1551, -0.1128, -0.0843],\n",
    "          [ 0.1710,  0.0692,  0.0509],\n",
    "          [-0.0590, -0.1110, -0.0164]]]]), Parameter containing:\n",
    "tensor([-0.1353,  0.0800,  0.1498, -0.0624,  0.0849,  0.0797,  0.0854,\n",
    "         0.0087, -0.0305, -0.1355, -0.1282, -0.1052,  0.1405,  0.0670,\n",
    "        -0.0697,  0.1293, -0.1096, -0.1419, -0.0726, -0.0485,  0.0210,\n",
    "        ...,\n",
    "        ...,\n",
    "        ...,\n",
    "tensor(1.00000e-02 *\n",
    "       [[-0.5404, -0.0259,  1.9778,  ..., -1.9308, -2.7125, -1.0087],\n",
    "        [ 4.1271, -3.8561,  2.4392,  ..., -3.1562,  2.2980,  2.0409],\n",
    "        [ 1.1132,  4.2947,  0.2672,  ...,  4.0022,  2.3223,  2.3442],\n",
    "        ...,\n",
    "        [-1.4538,  0.7068, -2.6589,  ...,  0.2066,  2.7136, -3.5039],\n",
    "        [-1.6343,  3.4130, -3.1040,  ..., -3.1541, -2.2489, -3.8731],\n",
    "        [ 1.1431,  2.3393, -0.9175,  ...,  0.6463,  1.1275,  3.7512]]), Parameter containing:\n",
    "tensor(1.00000e-02 *\n",
    "       [ 3.3374, -3.2318, -0.1811, -1.8821,  3.4910, -2.5810, -0.7791,\n",
    "        -3.0502,  3.1986, -0.2088])]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 就是成堆成堆的tensor，应该就是权值tensor。不太好看，我们来换个方式打印\n",
    "- 所以Conv2d一族[0, 3, 7, 10, 14, 17, 20, 23, 27, 30, 33, 36, 40, 43, 46, 49]  Conv2d*16\n",
    "- BatchNorm2d[1, 4, 8, 11, 15, 18, 21, 24, 28, 31, 34, 37, 41, 44, 47, 50]\n",
    "- ReLU[2, 5, 9, 12, 16, 19, 22, 25, 29, 32, 35, 38, 42, 45. 48, 51]\n",
    "- MaxPool2d[6, 13, 26, 39, 52]\n",
    "- AvgPool2d[53]\n",
    "```python\n",
    "Conv2d一族[0, 3, 7, 10, 14, 17, 20, 23, 27, 30, 33, 36, 40, 43, 46, 49]    16*2\n",
    "BatchNorm2d[1, 4, 8, 11, 15, 18, 21, 24, 28, 31, 34, 37, 41, 44, 47, 50]  16*2\n",
    "ReLU[2, 5, 9, 12, 16, 19, 22, 25, 29, 32, 35, 38, 42, 45. 48, 51]         16*0\n",
    "MaxPool2d[6, 13, 26, 39, 52]                                               5*\n",
    "AvgPool2d[53]                                                              1*  \n",
    "['C', 'C', 'M', 'C', 'C', 'M', 'C', 'C', 'C', 'C', 'M', 'C', 'C', 'C', 'C', 'M', 'C', 'C', 'C', 'C', 'M', 'C', 'C', ]\n",
    "```\n",
    "```python\n",
    "print(len(list(net.parameters())), '\\n', [t.shape for t in list(net.parameters())])\n",
    "net.module.features[*]._parameters.keys()\n",
    "net.module.features[*]\n",
    "66                                #(共54层) 所以Conv2d一族[0,3,]\n",
    "[\n",
    "    torch.Size([64, 3, 3, 3]),    #(0) weight, WNormConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    torch.Size([64]),             #(0) bias,   WNormConv2d\n",
    "    torch.Size([64]),             #(1) weight, BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    torch.Size([64]),             #(1) bias,   BatchNorm2d                   !!!!!!第(2)层的ReLU(inplace)没有参数\n",
    "    torch.Size([64, 64, 3, 3]),   #(3) weight, Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    torch.Size([64]),             #(0) bias,   Conv2d\n",
    "    torch.Size([64]), \n",
    "    torch.Size([64]), \n",
    "    torch.Size([128, 64, 3, 3]),  #(7)\n",
    "    torch.Size([128]), \n",
    "    torch.Size([128]), \n",
    "    torch.Size([128]), \n",
    "    torch.Size([128, 128, 3, 3]), #(10)\n",
    "    torch.Size([128]), \n",
    "    torch.Size([128]), \n",
    "    torch.Size([128]), \n",
    "    torch.Size([256, 128, 3, 3]), #(14)\n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256, 256, 3, 3]), #(17)\n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256, 256, 3, 3]), #(20)\n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256, 256, 3, 3]), #(23)\n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([256]), \n",
    "    torch.Size([512, 256, 3, 3]), #(27)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(30)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(33)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(36)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(40)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(43)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(46)\n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512]), \n",
    "    torch.Size([512, 512, 3, 3]), #(49) weight, Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    torch.Size([512]),            #(49) bias,   Conv2d\n",
    "    torch.Size([512]),            #(50) weight, BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    torch.Size([512]),            #(50) bias,   BatchNorm2d\n",
    "    torch.Size([10, 512]), \n",
    "    torch.Size([10])\n",
    "]\n",
    "```\n",
    "- ### 更进一步, *Optimizer.py*拿到'params'后，和'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov'一起组成了*Optimizer.py*自己的optimizer.param_groups[0]\n",
    "```python\n",
    "(Pdb) len(optimizer.param_groups)\n",
    "1\n",
    "(Pdb) optimizer.param_groups[0].keys()\n",
    "dict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov'])\n",
    "(Pdb) optimizer.param_groups[0]['params']==list(net.parameters())\n",
    "True\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 要想精准的观察WNormConv2d内参数的组织情况，有两个办法\n",
    "    1. #### 对照Sequential和list(net.parameters())推知shape为[64, 3, 3, 3]是想要的，缺点是有了变动不好察觉和对应\n",
    "    2. #### 循着```net._modules['features'][0]```(GPU cuda上稍有不同,外面多包了两层, 为\n",
    "        - ```net._modules['module']._modules['features'][0]```或者\n",
    "        - ```net._modules['module'].features[0]```或者\n",
    "        - ```net.module.features[0]```, \n",
    "        \n",
    " 其实这时```net._modules['module']===net.module```就是```models.vgg.VGG```，我们有\n",
    "\n",
    "```python\n",
    "(Pdb cuda) torch.typename(net._modules['module'])\n",
    "'models.vgg.VGG'\n",
    "(Pdb cuda) torch.typename(net.module)\n",
    "'models.vgg.VGG'\n",
    "(Pdb cuda) net.module.features[0]\n",
    "WNormConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "(Pdb) net._modules['features'][0].__dict__\n",
    "{'_backend': <torch.nn.backends.thnn.THNNFunctionBackend object at 0x7fba843645c0>, '_parameters': OrderedDict([('weight', Parameter containing:\n",
    "tensor([[[[-0.0191, -0.0400,  0.1563],\n",
    "          [ 0.0556, -0.1315, -0.0170],\n",
    "          [ 0.0995, -0.0993, -0.0222]],\n",
    "          ...,  torch.Size([64, 3, 3, 3])\n",
    "          ...,\n",
    "         [[-0.0766, -0.0551, -0.0710],\n",
    "          [-0.0923,  0.1289,  0.1604],\n",
    "          [-0.1634, -0.0955, -0.0720]]]])), ('bias', Parameter containing:\n",
    "tensor([-0.1851, -0.1273,  0.0364, -0.1920,  0.0549, -0.1784, -0.1878,\n",
    "        -0.1502, -0.1479,  0.0729,  0.1485,  0.1089,  0.0782, -0.0572,\n",
    "        -0.0653, -0.0015,  0.0976, -0.0060,  0.1051, -0.0174, -0.1549,\n",
    "         0.0412, -0.1924, -0.1846, -0.1177, -0.1213,  0.1412,  0.1261,\n",
    "         0.1845,  0.0469, -0.1450, -0.1656,  0.1338, -0.1886,  0.1317,\n",
    "         0.0341, -0.1478, -0.1509, -0.1823, -0.1858, -0.1722, -0.0852,\n",
    "        -0.1349, -0.0245,  0.1856, -0.1220, -0.0687,  0.0933, -0.0139,\n",
    "        -0.1610, -0.1683,  0.0315, -0.0984,  0.1578,  0.1015, -0.1262,\n",
    "         0.0571,  0.0220,  0.0429,  0.1241,  0.0154,  0.1352, -0.1696,\n",
    "        -0.0496]))]), '_buffers': OrderedDict(), '_backward_hooks': OrderedDict(), '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_modules': OrderedDict(), 'training': True, 'in_channels': 3, 'out_channels': 64, 'kernel_size': (3, 3), 'stride': (1, 1), 'padding': (1, 1), 'dilation': (1, 1), 'transposed': False, 'output_padding': (0, 0), 'groups': 1}\n",
    "(Pdb) net._modules['features'][0]._parameters['weight'].shape\n",
    "torch.Size([64, 3, 3, 3])\n",
    "(Pdb) net._modules['features'][0]._parameters['bias'].shape\n",
    "torch.Size([64])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### 继续看optimizer实例化之后的样子\n",
    "- ### 中间...省略处为 [net.parameters()](#net.parameters())，\n",
    "- 如前所述，\n",
    "    - net.parameters()-->\n",
    "    - *Optimizer.py*的'params'-->\n",
    "    - *Optimizer.py*的optimizer.param_groups[0]\n",
    "    \n",
    "```python\n",
    "(Pdb) optimizer.__dict__\n",
    "{\n",
    "    'defaults': {\n",
    "        'lr': 0.1, \n",
    "        'momentum': 0.9, \n",
    "        'dampening': 0, \n",
    "        'weight_decay': 0.0005, \n",
    "        'nesterov': False\n",
    "    }, \n",
    "    'state': defaultdict(<class 'dict'>, {}), \n",
    "    'param_groups': [{\n",
    "        'params': [\n",
    "            Parameter containing:tensor([]),\n",
    "            ...,net.parameters(),\n",
    "            Parameter containing:tensor([]),\n",
    "        ], \n",
    "        'lr': 0.1, \n",
    "        'momentum': 0.9, \n",
    "        'dampening': 0, \n",
    "        'weight_decay': 0.0005, \n",
    "        'nesterov': False\n",
    "    }]\n",
    "}\n",
    "(Pdb) torch.typename(optimizer.param_groups[0]['params'])\n",
    "'list'\n",
    "(Pdb) type(optimizer.param_groups[0]['params'])\n",
    "<class 'list'>\n",
    "(Pdb) type(optimizer.param_groups[0]['params'][0])\n",
    "<class 'torch.nn.parameter.Parameter'>\n",
    "(Pdb) torch.typename(optimizer.param_groups[0]['params'][0])\n",
    "'torch.FloatTensor'\n",
    "(Pdb) optimizer.param_groups[0]['params'][0].shape\n",
    "torch.Size([64, 3, 3, 3])\n",
    "(Pdb) optimizer.param_groups[0]['params'][0].__dict__\n",
    "{}\n",
    "(Pdb) p optimizer.param_groups[0]['params'][0].grad\n",
    "None\n",
    "```\n",
    "- ## 常用 ```optimizer.param_groups[0]['params'][0].__dict__ / grad```\n",
    "- ## 常用 ```net.module.features[0].parameters.__dict__```\n",
    "- ## 常用 ```p list(net.parameters())``` 这里一定要小心pdb的大坑! list会被当成pdb的命令!所以一定要先加p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## output 为什么是 128*10 啊?\n",
    "```python\n",
    "outputs.shape\n",
    "torch.Size([128, 10])\n",
    "```\n",
    "- 128是batch size， 10是分10个类\n",
    "- 来自vgg.py 22行 \n",
    "```python\n",
    "self.classifier = nn.Linear(512, 10)\n",
    "--> /home/xianr/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py(43)__init__()\n",
    "self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "self.bias = Parameter(torch.Tensor(out_features))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters 大探秘！\n",
    "\n",
    "## 我去，Parameters就是个彻头彻尾的Tensor，继承自Tensor，穿了层衣服，多了```__new__```和```__repr__```两个方法，没了， 连```__init__```都懒得重写。\n",
    "\n",
    "- ```__new__```：制造那个self的存在，object类定义的。参考[python 类中__new__ 和 __init__方法区别](https://zhuanlan.zhihu.com/p/21379984)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Foo'>\n",
      "<__main__.Foo object at 0x7fd5aa266080>\n",
      "<__main__.Foo object at 0x7fd5aa266080>\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "class Foo(object):\n",
    "    price = 50\n",
    "\n",
    "    def __new__(cls, *agrs, **kwds):\n",
    "        inst = object.__new__(cls, *agrs, **kwds)\n",
    "        print(cls)\n",
    "        print(inst)\n",
    "        return inst\n",
    "\n",
    "\n",
    "    def how_much_of_book(self, n):\n",
    "        print(self)\n",
    "        return self.price * n\n",
    "\n",
    "foo = Foo()\n",
    "print(foo.how_much_of_book(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```__repr__```：打印debug时候会显示的小提示，比如类型。参考[Python中__repr__和__str__区别](https://blog.csdn.net/luckytanggu/article/details/53649156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Test object at 0x7fd5aa2663c8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Test at 0x7fd5aa2663c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test(object):\n",
    "    def __init__(self, value='hello, world!'):\n",
    "        self.data = value\n",
    "\n",
    "t = Test()\n",
    "print(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters特殊就特殊在它的子类身份，因为Module类不认Tensor，就认Parameters，看到是Parameter的会多次区别对待\n",
    "- /home/xianr/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(541)__setattr__()\n",
    "- /home/xianr/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py(655)_load_from_state_dict()\n",
    "Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
    "    very special property when used with :class:`Module` s - when they're\n",
    "    assigned as Module attributes they are automatically added to the list of\n",
    "    its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
    "    Assigning a Tensor doesn't have such effect. This is because one might\n",
    "    want to cache some temporary state, like last hidden state of the RNN, in\n",
    "    the model. If there was no such class as :class:`Parameter`, these\n",
    "    temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
